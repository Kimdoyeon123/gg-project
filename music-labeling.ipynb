{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "import numpy as np\n",
    "import json\n",
    "import os\n",
    "\n",
    "# 1. JSON 파일에서 데이터를 불러오기\n",
    "json_file_path = 'audio_features_data.json'  # json 파일의 이름이 실제 파일 이름과 일치해야 합니다\n",
    "\n",
    "# JSON 파일 읽기\n",
    "with open(json_file_path, 'r') as f:\n",
    "    audio_features_data = json.load(f)\n",
    "\n",
    "# values()를 사용하여 데이터만 추출하고 numpy array로 변환\n",
    "data = np.array(list(audio_features_data.values()))\n",
    "\n",
    "# 분위기 레이블: 예시로 3개의 레이블을 부여 (각 노래에 실제 레이블을 할당해야 합니다)\n",
    "# 예시 레이블, 실제 데이터에 맞게 수정하세요\n",
    "num_samples = len(data)\n",
    "\n",
    "labels = [0] * 100 + [1] * 69 + [2] * (num_samples - 169)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.20073e+02, 3.63000e-01, 7.47000e-01, ..., 7.30000e-03,\n",
       "        8.15000e-02, 7.38000e-02],\n",
       "       [1.24994e+02, 7.57000e-01, 8.95000e-01, ..., 8.16000e-02,\n",
       "        6.23000e-02, 1.54000e-01],\n",
       "       [1.19986e+02, 8.01000e-01, 8.86000e-01, ..., 1.30000e-01,\n",
       "        1.85000e-01, 3.09000e-01],\n",
       "       ...,\n",
       "       [8.70930e+01, 5.71000e-01, 5.85000e-01, ..., 6.24000e-01,\n",
       "        6.10000e-02, 1.25000e-01],\n",
       "       [8.00740e+01, 1.75000e-01, 4.50000e-01, ..., 6.57000e-01,\n",
       "        2.95000e-02, 1.39000e-01],\n",
       "       [2.01801e+02, 3.68000e-01, 6.45000e-01, ..., 4.39000e-01,\n",
       "        3.00000e-02, 1.49000e-01]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/100], Loss: 2.9707\n",
      "Epoch [2/100], Loss: 0.7226\n",
      "Epoch [3/100], Loss: 1.2376\n",
      "Epoch [4/100], Loss: 0.8248\n",
      "Epoch [5/100], Loss: 0.1408\n",
      "Epoch [6/100], Loss: 0.1930\n",
      "Epoch [7/100], Loss: 0.4995\n",
      "Epoch [8/100], Loss: 1.2547\n",
      "Epoch [9/100], Loss: 0.1523\n",
      "Epoch [10/100], Loss: 0.2480\n",
      "Epoch [11/100], Loss: 1.1154\n",
      "Epoch [12/100], Loss: 0.9222\n",
      "Epoch [13/100], Loss: 1.6223\n",
      "Epoch [14/100], Loss: 0.1510\n",
      "Epoch [15/100], Loss: 0.2264\n",
      "Epoch [16/100], Loss: 0.5008\n",
      "Epoch [17/100], Loss: 1.3896\n",
      "Epoch [18/100], Loss: 0.4421\n",
      "Epoch [19/100], Loss: 0.3111\n",
      "Epoch [20/100], Loss: 1.1300\n",
      "Epoch [21/100], Loss: 2.2384\n",
      "Epoch [22/100], Loss: 0.5805\n",
      "Epoch [23/100], Loss: 0.5843\n",
      "Epoch [24/100], Loss: 0.1054\n",
      "Epoch [25/100], Loss: 0.1937\n",
      "Epoch [26/100], Loss: 0.1628\n",
      "Epoch [27/100], Loss: 2.0857\n",
      "Epoch [28/100], Loss: 0.3001\n",
      "Epoch [29/100], Loss: 0.4065\n",
      "Epoch [30/100], Loss: 1.6345\n",
      "Epoch [31/100], Loss: 0.7442\n",
      "Epoch [32/100], Loss: 0.0548\n",
      "Epoch [33/100], Loss: 0.2776\n",
      "Epoch [34/100], Loss: 0.3488\n",
      "Epoch [35/100], Loss: 0.1427\n",
      "Epoch [36/100], Loss: 0.2033\n",
      "Epoch [37/100], Loss: 0.1529\n",
      "Epoch [38/100], Loss: 0.2505\n",
      "Epoch [39/100], Loss: 0.3128\n",
      "Epoch [40/100], Loss: 0.4171\n",
      "Epoch [41/100], Loss: 1.6185\n",
      "Epoch [42/100], Loss: 0.0837\n",
      "Epoch [43/100], Loss: 0.6780\n",
      "Epoch [44/100], Loss: 0.7911\n",
      "Epoch [45/100], Loss: 0.1654\n",
      "Epoch [46/100], Loss: 2.8829\n",
      "Epoch [47/100], Loss: 0.2451\n",
      "Epoch [48/100], Loss: 0.0536\n",
      "Epoch [49/100], Loss: 3.1673\n",
      "Epoch [50/100], Loss: 0.0570\n",
      "Epoch [51/100], Loss: 0.4061\n",
      "Epoch [52/100], Loss: 0.1035\n",
      "Epoch [53/100], Loss: 2.5799\n",
      "Epoch [54/100], Loss: 3.4109\n",
      "Epoch [55/100], Loss: 1.0251\n",
      "Epoch [56/100], Loss: 0.1402\n",
      "Epoch [57/100], Loss: 0.4632\n",
      "Epoch [58/100], Loss: 0.1739\n",
      "Epoch [59/100], Loss: 0.6980\n",
      "Epoch [60/100], Loss: 0.7995\n",
      "Epoch [61/100], Loss: 0.7692\n",
      "Epoch [62/100], Loss: 0.2335\n",
      "Epoch [63/100], Loss: 0.1606\n",
      "Epoch [64/100], Loss: 0.1577\n",
      "Epoch [65/100], Loss: 1.5503\n",
      "Epoch [66/100], Loss: 0.3221\n",
      "Epoch [67/100], Loss: 0.8050\n",
      "Epoch [68/100], Loss: 0.8118\n",
      "Epoch [69/100], Loss: 0.7398\n",
      "Epoch [70/100], Loss: 0.1062\n",
      "Epoch [71/100], Loss: 0.0664\n",
      "Epoch [72/100], Loss: 0.5914\n",
      "Epoch [73/100], Loss: 0.1226\n",
      "Epoch [74/100], Loss: 0.1614\n",
      "Epoch [75/100], Loss: 0.8543\n",
      "Epoch [76/100], Loss: 0.8833\n",
      "Epoch [77/100], Loss: 0.4455\n",
      "Epoch [78/100], Loss: 2.6646\n",
      "Epoch [79/100], Loss: 0.0682\n",
      "Epoch [80/100], Loss: 0.0711\n",
      "Epoch [81/100], Loss: 0.1319\n",
      "Epoch [82/100], Loss: 0.4514\n",
      "Epoch [83/100], Loss: 0.2124\n",
      "Epoch [84/100], Loss: 0.0799\n",
      "Epoch [85/100], Loss: 0.5094\n",
      "Epoch [86/100], Loss: 0.6112\n",
      "Epoch [87/100], Loss: 0.9909\n",
      "Epoch [88/100], Loss: 0.6500\n",
      "Epoch [89/100], Loss: 0.2324\n",
      "Epoch [90/100], Loss: 1.1334\n",
      "Epoch [91/100], Loss: 0.1304\n",
      "Epoch [92/100], Loss: 0.5814\n",
      "Epoch [93/100], Loss: 2.3027\n",
      "Epoch [94/100], Loss: 0.9805\n",
      "Epoch [95/100], Loss: 0.1417\n",
      "Epoch [96/100], Loss: 0.1177\n",
      "Epoch [97/100], Loss: 0.1624\n",
      "Epoch [98/100], Loss: 0.0127\n",
      "Epoch [99/100], Loss: 0.7128\n",
      "Epoch [100/100], Loss: 0.0383\n"
     ]
    }
   ],
   "source": [
    "# 2. PyTorch Dataset 클래스 정의\n",
    "class MusicDataset(Dataset):\n",
    "    def __init__(self, data, labels):\n",
    "        self.data = torch.tensor(data, dtype=torch.float32)\n",
    "        self.labels = torch.tensor(labels, dtype=torch.long)\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        return self.data[index], self.labels[index]\n",
    "\n",
    "# 데이터 차원 확장 (CNN 입력으로 2D 필요, Batch x Channel x Height x Width)\n",
    "data = data[:, np.newaxis, np.newaxis, :]  # (batch_size, 1, 1, 7) 형태로 변경\n",
    "\n",
    "# PyTorch Dataset과 DataLoader 정의는 기존 코드와 동일하게 사용\n",
    "dataset = MusicDataset(data, labels)\n",
    "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)\n",
    "\n",
    "\n",
    "# 3. 간단한 CNN 모델 정의\n",
    "class SimpleCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1, 1))  # 커널 크기 변경\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1, 1))\n",
    "        self.fc1 = nn.Linear(32 * 7, 64)  # 피쳐맵 크기를 일자로 펼치기 (1 x 7의 결과가 32채널이므로 32*7)\n",
    "        self.fc2 = nn.Linear(64, 3)  # 분위기 레이블 수 (예: 3가지 레이블)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = nn.ReLU()(self.conv1(x))\n",
    "        x = nn.MaxPool2d((1, 1))(x)  # MaxPooling은 (1, 1)으로 설정하여 크기 변경하지 않음\n",
    "        x = nn.ReLU()(self.conv2(x))\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = nn.ReLU()(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# 4. 모델, 손실 함수 및 옵티마이저 설정\n",
    "model = SimpleCNN()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# 5. 모델 학습\n",
    "num_epochs = 100\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    for inputs, labels in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item():.4f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델이 저장되었습니다: cnn_learning.pth\n"
     ]
    }
   ],
   "source": [
    "# 모델 저장 경로 설정\n",
    "model_save_path = \"cnn_learning.pth\"\n",
    "\n",
    "# 모델의 state_dict를 저장\n",
    "torch.save(model.state_dict(), model_save_path)\n",
    "print(f\"모델이 저장되었습니다: {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleCNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(1, 1), stride=(1, 1))\n",
       "  (fc1): Linear(in_features=224, out_features=64, bias=True)\n",
       "  (fc2): Linear(in_features=64, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 모델을 새로 초기화하고 state_dict 불러오기\n",
    "loaded_model = SimpleCNN()  # 모델 구조는 동일해야 합니다\n",
    "loaded_model.load_state_dict(torch.load(model_save_path))\n",
    "loaded_model.eval()  # 평가 모드로 변경 (드롭아웃, 배치 정규화 등 비활성화)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Label: 0\n"
     ]
    }
   ],
   "source": [
    "# 예시 입력 데이터 (학습 시 사용했던 형식과 동일한 형태로 준비해야 함)\n",
    "sample_data = np.array([[\n",
    "        116.08,\n",
    "        0.834,\n",
    "        0.876,\n",
    "        0.855,\n",
    "        0.00857,\n",
    "        0.166,\n",
    "        0.148\n",
    "    ]])  # 예시 데이터\n",
    "sample_data = sample_data[:, np.newaxis, np.newaxis, :]  # (1, 1, 1, 7) 형태로 변환\n",
    "sample_data_tensor = torch.tensor(sample_data, dtype=torch.float32)\n",
    "\n",
    "# 모델로 예측하기\n",
    "with torch.no_grad():  # 예측 시에는 gradient 계산이 필요하지 않으므로 no_grad() 사용\n",
    "    prediction = loaded_model(sample_data_tensor)\n",
    "    predicted_label = torch.argmax(prediction, dim=1).item()\n",
    "\n",
    "print(f\"Predicted Label: {predicted_label}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
